{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2Ag36GMN7_2"
      },
      "source": [
        "# **Building the DataLoader**\n",
        "\n",
        "We build the dataloader to enable easier and more systematic access of our images and captions for the actual CNN and RNN models.\n",
        "\n",
        "First, we import all the necessary libraries for out Dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qyk6u__eQUKJ",
        "outputId": "971994a2-ff35-4f4a-b5f2-38690342c80f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CSj9cJ8vUFjh"
      },
      "outputs": [],
      "source": [
        "# %cd /content/drive/MyDrive/\n",
        "# !unzip image_captions.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXty-aA0-8v7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torchvision import transforms\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ7btsl4OUDW"
      },
      "source": [
        "Here we create 2 variables for storing the loaction of the caption and image folders from the dataset. The ```spacy_eng``` is made for later use in tokenising the captions as it has english datasets which enable it for better tokenisation around punctuation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZFpmjfbPQyX"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/image_captions')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bF3D1mU0ONYJ"
      },
      "outputs": [],
      "source": [
        "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "img_folder = r\"/content/drive/MyDrive/image_captions/Images\"\n",
        "caption_file = r\"/content/drive/MyDrive/image_captions/captions.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjnVP2ZFEZ8R"
      },
      "source": [
        "I had created a function `read_data` to read the data from the captions.txt file and store the image id's and captions in a dictionary with each image id as key pointing to all 5 captions for that image in a list as the value. But I had to discard that and use `pd.read_csv`because I got an error of size mismatch in the `forward` of `DecoderRNN` class while concatenating the features and embeddings tensors(also mentioned in that block) because with all 5 captions together for each image provided as the dataset for Dataloader, it generated the captions as a (5, batch_size, max_len) dimensioned tensor, but for concatenating, I needed it to be a (batch_size, max_len) tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWNFzkqaFolx"
      },
      "outputs": [],
      "source": [
        "# # Extracting data from Captions file\n",
        "# def read_data(caption_file):\n",
        "#     img_captions = {}\n",
        "\n",
        "#     with open(caption_file, \"r\") as f:\n",
        "#         next(f)\n",
        "#         cap = f.readlines()\n",
        "\n",
        "#     # Mapping image ID's with their captions\n",
        "#     for line in cap:\n",
        "\n",
        "#         line = line.split(',')\n",
        "#         img_id, caption = line[0], line[1:]\n",
        "#         caption = \",\".join(caption)                 # if any caption had commas, combining the split caption\n",
        "#         if img_id not in img_captions:\n",
        "#             img_captions[img_id] = []\n",
        "#         img_captions[img_id].append(caption)\n",
        "\n",
        "#     return img_captions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QUIStqWSUDn"
      },
      "source": [
        "Now we create another function `caption_cleaner` which is used to preprocess our stored captions. It lowercases all characters, removes all punctuations, all non-alphabetical characters, and all multiple spaces from the captions. It also adds the `<SOS>` and the `<EOS>` token at the start and end of each caption respectively, representing the start and the end of sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCgj2vCuOO1K"
      },
      "outputs": [],
      "source": [
        "# Preprocess caption for easier implementation\n",
        "def caption_cleaner(captions):\n",
        "    for i in range(len(captions)):\n",
        "        caption = captions[i]\n",
        "        caption = caption.lower().strip()           # lowercase all characters\n",
        "        caption = caption.replace(\"\\\"\", '')\n",
        "        caption = caption.replace('[^a-z]' , '')    # removes all special characters\n",
        "        caption = caption.replace('\\s+', ' ')       # replaces multiple spaces by single space\n",
        "        caption = f\"<SOS> {caption} <EOS>\"\n",
        "        captions[i] = caption\n",
        "    return captions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM77JX-CSxRR"
      },
      "source": [
        "### **Creating a Vocabulary**\n",
        "We create a vocabulary to store all of the words used in the captions into a dictionary, thus forming a mini-dictionary(not just a python dictionary, but an actual one) of words. We used this vocabulary to assign indexes to each and every word, allowing us to convert these words to their specific indexes and those indexes back into words using the two dictionaries `string_to_index` and `index_to_string`.\n",
        "\n",
        "We do this by creating the class **vocab**, which contains the two dictionaries, and pass to it the frequency threshold, which specifies the minimum number of times a word should repeat in the captions for it to actually be considered in the dictionary. We have a `__len__()` method which provides us the length of the vocabulary, the `tokenizer_eng` method(as a static method because it does not need any parameters of the vocab class to work, but only the text provided) to tokenize the captions, and the `numericalize` method which can convert a tokenized caption into a list containing numbers which uniquely indentifies each caption to be evaluated by the captioning model. Lastly, the `build_vocab` method allows us to actually build the vocabulary. It takes each word from caption and checks if it meets the reqired freq_threshold, and then appends it to the dictoinaries with unique indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEvz8FndOO_I"
      },
      "outputs": [],
      "source": [
        "class vocab():\n",
        "    def __init__(self, freq_thresh):\n",
        "        self.string_to_index = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.index_to_string = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.freq_thresh = freq_thresh\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.string_to_index)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer_eng(text):\n",
        "        return [word for word in spacy_eng.tokenizer(text)]\n",
        "\n",
        "    def build_vocab(self, captions):\n",
        "        freq = {}\n",
        "        idx = 4\n",
        "\n",
        "        for sentence in captions:\n",
        "            for word in self.tokenizer_eng(sentence):\n",
        "                if word not in freq:\n",
        "                    freq[word] = 1\n",
        "                else:\n",
        "                    freq[word] += 1\n",
        "\n",
        "                if freq[word] >= self.freq_thresh:\n",
        "                    self.string_to_index[word] = idx\n",
        "                    self.index_to_string[idx] = word\n",
        "                    index += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokenized_txt = self.tokenizer_eng(text)\n",
        "        return [\n",
        "            self.string_to_index[token] if token in self.string_to_index else self.string_to_index[\"<UNK>\"]\n",
        "            for token in tokenized_txt\n",
        "        ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71Ogq0V4WiYb"
      },
      "source": [
        "### **Creating the Dataset**\n",
        "Finally, we create the dataset which allows us to access all the images with their respective captions, all in form of pytorch tensors.\n",
        "\n",
        "The **dataset8k** class inherits from the `torch.utils.data.Dataset` class. We provide the path to the image folder, caption file, any transform if required, and a default freq_threshold of 5 for creating the vocabulary. It reads the data from the captions file and then it cleans the captions.It creates a vocabulary for the read data.Under the `__getitem__` method, it takes each image from the read dataset and applies any transform if provided, then applies the numericalize method on the each of the 5 corresponding captions for the image.\n",
        "\n",
        "We also pad each caption at the end upto a specified max length of 50 words, with the `<PAD>` token whose string_to_index value is 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KiHvXenOPFc"
      },
      "outputs": [],
      "source": [
        "class dataset8k(Dataset):\n",
        "    def __init__(self, img_folder, caption_file, transform = None, freq_thresh = 5):\n",
        "        self.img_folder = img_folder\n",
        "        self.dataset = pd.read_csv(caption_file)\n",
        "        self.transform  = transform\n",
        "\n",
        "        # Getting lists for image_ids and captions\n",
        "        self.imgs = self.dataset[\"image\"]\n",
        "        self.captions = self.dataset[\"caption\"]\n",
        "        self.captions = caption_cleaner(self.captions)\n",
        "\n",
        "        # Creating a vocabulary\n",
        "        self.vocab = vocab(freq_thresh)\n",
        "        self.vocab.build_vocab([cap for cap in self.captions])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_id = self.imgs[index]\n",
        "        caption = self.captions[index]\n",
        "        img = Image.open(os.path.join(self.img_folder, img_id)).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        numericalized_caption = self.vocab.numericalize(caption)\n",
        "        return img, torch.tensor(numericalized_caption)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQS3uM2gcOn1"
      },
      "source": [
        "We define `MyCollate` class to arrange the images and captions in form of sequential batches, while also applying formatting such as padding each caption upto the max length of caption for each batch, using `pad_sequence` function in torch utils. `MyCollate` is passed as an argument to the `DataLoader` which then actually performs the batching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "si2DKffKOPI1"
      },
      "outputs": [],
      "source": [
        "class MyCollate():\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        imgs = [torch.unsqueeze(item[0], 0) for item in batch]\n",
        "        imgs = torch.cat(imgs, dim = 0)\n",
        "        cap = [item[1] for item in batch]\n",
        "        cap = pad_sequence(cap, batch_first = False, padding_value = self.pad_idx)\n",
        "\n",
        "        return imgs, cap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjKrOwTMabli"
      },
      "source": [
        "### **Creating The Loader Fucntion**\n",
        "To finish of the Dataloader, we create the `get_loader` function which takes all the parameters of dataset class, along with some hyperparameters like batch size and num of workers. We also provide the val and test split(the fraction of the dataset forming val set and test set) to split the dataset into three train-val-test datasets. It then creates the dataset and splits it into the train-val-test sets. Then it creates the train_loader, val_loader and test_loader, each with the mentioned `batch_size` and `num_workers`.\n",
        "\n",
        "It returns the train, val, test loaders as a 3 length dictionary with keys --> `'train', 'val', 'test'`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIkh0QZeOPMN"
      },
      "outputs": [],
      "source": [
        "def get_loader(\n",
        "        img_folder,\n",
        "        caption_file,\n",
        "        transform,\n",
        "        val_split,\n",
        "        test_split,\n",
        "        batch_size = 32,\n",
        "        num_workers = 2,\n",
        "        shuffle = True,\n",
        "        pin_memory = False,\n",
        "):\n",
        "    dataset = dataset8k(img_folder, caption_file, transform = transform)\n",
        "    pad_idx = dataset.vocab.string_to_index[\"<PAD>\"]\n",
        "\n",
        "    train_idx, val_test_idx = train_test_split(list(range(len(dataset))), test_size=(val_split + test_split))\n",
        "    val_idx, test_idx = train_test_split(list(range(len(val_test_idx))), test_size=test_split)\n",
        "\n",
        "    train_set = Subset(dataset, train_idx)\n",
        "    val_set = Subset(dataset, val_idx)\n",
        "    test_set = Subset(dataset, test_idx)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_set,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        shuffle=shuffle,\n",
        "        pin_memory=pin_memory,\n",
        "        collate_fn= MyCollate(pad_idx),\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        dataset=val_set,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        shuffle=shuffle,\n",
        "        pin_memory=pin_memory,\n",
        "        collate_fn= MyCollate(pad_idx),\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        dataset=test_set,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        shuffle=shuffle,\n",
        "        pin_memory=pin_memory,\n",
        "        collate_fn= MyCollate(pad_idx),\n",
        "    )\n",
        "    return {'train': train_loader, 'val': val_loader, 'test': test_loader}, dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7u7JStueNNn"
      },
      "source": [
        "  This is a sample code to check the working of the dataloader, with an eg. transform. It is important to note that the CNN Encoder we will be using later is based on the VGG16 architecture, which accpets images of size (3, 224, 224). So any transform we apply should have a layer that resizes or crops the image to (224, 224):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eg6M2jssOPgp"
      },
      "outputs": [],
      "source": [
        "# transform = transforms.Compose(\n",
        "#     [\n",
        "#     transforms.Resize((224,224)),\n",
        "#     transforms.ToTensor(),]\n",
        "# )\n",
        "\n",
        "# dataloader, _ = get_loader(\n",
        "#     img_folder=img_folder,\n",
        "#     caption_file=caption_file,\n",
        "#     transform = transform,\n",
        "#     val_split=0.2,\n",
        "#     test_split = 0.1,\n",
        "# )\n",
        "\n",
        "# for idx, (imgs, captions) in enumerate(dataloader['train']):\n",
        "#     print(imgs.shape)\n",
        "#     print(captions.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu-q2diDRVqu"
      },
      "source": [
        "---\n",
        "\n",
        "# **Creating The Model**\n",
        "\n",
        "For an image captioning model, we need to first identify the image features and then use those image features to identify the features of the caption, to calculate the loss function between the generated caption and the actual caption for backward propagation. So we need to implement two models, one to identify the image features and another to generate the captions from the features, and then link those two together to form the final caption."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjwcqqKBUT-x"
      },
      "source": [
        "### **EncoderCNN**\n",
        "\n",
        "To identify image features, we use a CNN model, which specialises in extracting image features. I have implemented the CNN based on the VGG16 architecture, which is one of the most popopular image recognition models.\n",
        "\n",
        "I have not used a pretrained VGG16 model, but implemented its structure with all the layers from it's paper. I have removed it's last SoftMax layer which is used for feature classification, because we just need the features of the image, not to classify them.\n",
        "\n",
        "NB: In the `forward` method of the model, we reshaped the image after applying all the conv layers because now we need to apply the fully connected linear layers, for which we need to flattemn the image into one dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7lnkC_bOPjs"
      },
      "outputs": [],
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size, num_classes = 1000):\n",
        "        super(EncoderCNN,self).__init__()\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.block4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.block5 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(7*7*512, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, num_classes),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(num_classes, embed_size))\n",
        "\n",
        "    def forward(self, image):\n",
        "        features = self.block1(image)\n",
        "        features = self.block2(features)\n",
        "        features = self.block3(features)\n",
        "        features = self.block4(features)\n",
        "        features = self.block5(features)\n",
        "        features = torch.reshape(features, (features.size(0), -1))\n",
        "        features = self.fc_layers(features)\n",
        "        return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--732V0rRbvh"
      },
      "source": [
        "### **DecoderRNN**\n",
        "For the caption generation, we use an RNN model, as it is made for analysing sequential data, such as lists, text(sentences), etc. We use specifically LSTM for this task as it can predict the next element in sequences from the previous element, which is really handy for caption generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96fYEYu4OPnc"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        embeddings = self.dropout(self.embed(captions))\n",
        "        features = torch.unsqueeze(features, 0)\n",
        "\n",
        "        # print(f\"{features.shape}\\t{embeddings.shape}\\n\")\n",
        "        '''was having an error where the features and embeddings weren't concateneting due to size incompatibility\n",
        "        so i tried printing their shapes to check where the error could have been '''\n",
        "\n",
        "        embeddings = torch.cat((features, embeddings), dim=0)\n",
        "        hiddens, _ = self.lstm(embeddings)\n",
        "        outputs = self.linear(hiddens)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy6L7pG6Z_lU"
      },
      "source": [
        "### **Combining CNN and RNN**\n",
        "Now, we need to find a way to attach the two models together so that, when we feed it an image, it can generate the respective caption in one go, by extracting the features and then linking the features to the caption.\n",
        "\n",
        "Here, we first provided the CNN with the image, then took the generated output features and the caption for the image, and fed it to the RNN. We have another function for caption generation which predicts the caption output, and converts it into words, and return the caption as a list of its tokens(or words)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRhuiCyjZ_VI"
      },
      "outputs": [],
      "source": [
        "class Image_Captioning(nn.Module):\n",
        "    def __init__(self, num_classes, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(Image_Captioning, self).__init__()\n",
        "        self.encoderCNN = EncoderCNN(embed_size, embed_size)\n",
        "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.encoderCNN(images)\n",
        "        outputs = self.decoderRNN(features, captions)\n",
        "        return outputs\n",
        "\n",
        "    def caption_return(self, images, vocab, max_len = 50):\n",
        "        with torch.no_grad():\n",
        "            feature = torch.unsqueeze(self.encoderCNN(images), 0)\n",
        "            token = None\n",
        "            predicted_captions = []\n",
        "\n",
        "            for _ in range(max_len):\n",
        "                hiddens, token = self.decoderRNN.lstm(feature, token)\n",
        "                output = self.decoderRNN.linear(torch.squeeze(hiddens, 0))\n",
        "                prediction = output.argmax(1)\n",
        "                feature = torch.unsqueeze(self.decoderRNN.embed(prediction), 0)\n",
        "                prediction = torch.unsqueeze(prediction, 1)\n",
        "\n",
        "                for i in range(len(prediction)):\n",
        "                    word_idx = prediction[i].item()\n",
        "                    if word_idx == vocab.string_to_index[\"<EOS>\"]:\n",
        "                        break\n",
        "                    elif word_idx in vocab.index_to_string:\n",
        "                        predicted_captions.append(word_idx)\n",
        "                    else:\n",
        "                        predicted_captions.append(vocab.string_to_index[\"<UNK>\"])\n",
        "            predicted_captions = torch.transpose(torch.reshape(torch.tensor(predicted_captions), (-1,images.size(0))), 0, 1)\n",
        "\n",
        "        return predicted_captions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0PJtLDHbBR0"
      },
      "source": [
        "The `caption_return` method here was the toughest to deal with, as while training the dataset, there were a bunch of errors popping up, and I had to run multiple times to identify every single one, while waiting for 5-10 min for all the code blocks to run before generating any output/error each time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJcInFFzbBPV"
      },
      "source": [
        "---\n",
        "\n",
        "# **Training the Model**\n",
        "\n",
        "For training, we first define our hyperparamters and the transforms we want to apply to the images. Here, I have resized the image to(224,224) because that is the image size VGG16 accepts. Then I converted the resized image to a tensor, and finally normalized the tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6ZpPbVNZ_PN"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "embed_size = 256\n",
        "hidden_size = 256\n",
        "batch_size = 32\n",
        "num_epochs = 2\n",
        "learning_rate = 1e-3\n",
        "num_layers = 10\n",
        "num_classes = 1000\n",
        "num_workers = 2\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-lG4wvo21z-"
      },
      "source": [
        "Then we create our dataloader that loads the images and captions after splitting the dataset into train-val-test batches. Then, we calculate the vocabulary size of our dataset, which is required by our DecoderRNN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjXVEp3Hb9A-"
      },
      "outputs": [],
      "source": [
        "dataloader, dataset = get_loader(\n",
        "    img_folder=img_folder,\n",
        "    caption_file=caption_file,\n",
        "    transform=transform,\n",
        "    val_split = 0.15,\n",
        "    test_split=0.1,\n",
        "    batch_size=batch_size,\n",
        "    num_workers= num_workers\n",
        ")\n",
        "vocabulary = dataset.vocab\n",
        "vocab_size = dataset.vocab.__len__()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZTwnTHk4GVo"
      },
      "source": [
        "We finally define the train function, which goes over our training dataset and feeds it to the Image_captioning model, which then calculates the loss function and then optimizes the model parameters to reduce the loss. For each epoch after training, we save the model's weights and biases in \"model_weights.pth\" file and the optimizer parameters in the \"optimizer.pth\" file so that the best optimized model at all stages is saved and we can load the model at any later stage to further optimize or use it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuHoWTRZb8-u"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, vocab_size):\n",
        "    train_loader = dataloader['train']\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model = Image_Captioning(\n",
        "        num_classes= num_classes,\n",
        "        embed_size=embed_size,\n",
        "        hidden_size=hidden_size,\n",
        "        vocab_size=vocab_size,\n",
        "        num_layers=num_layers,\n",
        "    )\n",
        "    # creaitng loss function and optimizer\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=vocabulary.string_to_index[\"<PAD>\"])\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        score = 0\n",
        "        total_loss = 0\n",
        "\n",
        "        for idx ,(imgs, captions) in enumerate(train_loader):\n",
        "            imgs = imgs.to(device)\n",
        "            captions = captions.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = 0\n",
        "\n",
        "            outputs = model(imgs, captions[:-1])\n",
        "            loss += loss_fn(torch.reshape(outputs, (-1, outputs.shape[2])), captions.reshape(-1))\n",
        "            loss /= batch_size\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            predicted_captions = model.caption_return(imgs, vocabulary)\n",
        "            for i in range(len(predicted_captions)):\n",
        "                score += sentence_bleu([[caption[i] for caption in captions]], predicted_captions[i])\n",
        "\n",
        "        avg_score = score/len(train_loader)\n",
        "        avg_loss = total_loss/len(train_loader)\n",
        "        print(f\"Avg Loss: {avg_loss}\\nAccuracy: {avg_score*100}%\")\n",
        "\n",
        "        torch.save(model.state_dict(), f\"model_weights{epoch + 1}.pth\")\n",
        "        torch.save(optimizer.state_dict(), f\"optimiser{epoch + 1}.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujM840Ygjj29",
        "outputId": "d85477c3-b80f-42c0-a8b1-1fc1e2e99f7f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        }
      ],
      "source": [
        "train(dataloader, vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZzcy1Ciz45J"
      },
      "source": [
        "# **Testing the Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hjhd4Rp-2jAL"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, vocab_size):\n",
        "    test_loader = dataloader['test']\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model = Image_Captioning(\n",
        "        num_classes= num_classes,\n",
        "        embed_size=embed_size,\n",
        "        hidden_size=hidden_size,\n",
        "        vocab_size=vocab_size,\n",
        "        num_layers=num_layers,\n",
        "    ).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model.load_state_dict(torch.load(\"model_weights.pth\"))\n",
        "    model.eval()\n",
        "    score = 0\n",
        "\n",
        "    for idx ,(imgs, captions) in enumerate(test_loader):\n",
        "        imgs = imgs.to(device)\n",
        "        captions = captions.to(device)\n",
        "\n",
        "        predicted_captions = model.caption_return(imgs, vocabulary)\n",
        "        for i in range(len(predicted_captions)):\n",
        "            score += sentence_bleu([[caption[i] for caption in captions]], predicted_captions[i])\n",
        "\n",
        "\n",
        "    avg_score = score/len(test_loader)\n",
        "    print(f\"Accuracy: {score*100}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
